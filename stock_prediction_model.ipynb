{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8688c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting plotly\n",
      "  Downloading plotly-6.3.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting yfinance\n",
      "  Using cached yfinance-0.2.65-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting textblob\n",
      "  Using cached textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.55.1-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.59.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (107 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-2.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Using cached multitasking-0.0.12-py3-none-any.whl\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in ./.venv/lib/python3.11/site-packages (from yfinance) (4.3.8)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Using cached frozendict-2.4.6-py311-none-any.whl.metadata (23 kB)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Using cached peewee-3.18.2-cp311-cp311-macosx_15_0_arm64.whl\n",
      "Collecting beautifulsoup4>=4.11.1 (from yfinance)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting curl_cffi>=0.7 (from yfinance)\n",
      "  Downloading curl_cffi-0.13.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting protobuf>=3.19.0 (from yfinance)\n",
      "  Using cached protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting websockets>=13.0 (from yfinance)\n",
      "  Using cached websockets-15.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting nltk>=3.9 (from textblob)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.3-cp311-cp311-macosx_10_9_universal2.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-21.0.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.12.15-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.7.34-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.24.0->datasets)\n",
      "  Downloading hf_xet-1.1.7-cp37-abi3-macosx_11_0_arm64.whl.metadata (703 bytes)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.7.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.6.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.20.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (73 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4>=4.11.1->yfinance)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting cffi>=1.12.0 (from curl_cffi>=0.7->yfinance)\n",
      "  Using cached cffi-1.17.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.12.0->curl_cffi>=0.7->yfinance)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting click (from nltk>=3.9->textblob)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "Using cached numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Using cached scikit_learn-1.7.1-cp311-cp311-macosx_12_0_arm64.whl (8.7 MB)\n",
      "Using cached matplotlib-3.10.5-cp311-cp311-macosx_11_0_arm64.whl (8.1 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading plotly-6.3.0-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m128.1 kB/s\u001b[0m  \u001b[33m0:01:00\u001b[0mm0:00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hUsing cached yfinance-0.2.65-py2.py3-none-any.whl (119 kB)\n",
      "Using cached textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp311-cp311-macosx_10_9_universal2.whl (204 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading transformers-4.55.1-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m289.3 kB/s\u001b[0m  \u001b[33m0:00:50\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m36m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.7-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached aiohttp-3.12.15-cp311-cp311-macosx_11_0_arm64.whl (471 kB)\n",
      "Using cached multidict-6.6.4-cp311-cp311-macosx_11_0_arm64.whl (44 kB)\n",
      "Using cached yarl-1.20.1-cp311-cp311-macosx_11_0_arm64.whl (89 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl (270 kB)\n",
      "Downloading curl_cffi-0.13.0-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached cffi-1.17.1-cp311-cp311-macosx_11_0_arm64.whl (178 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.59.0-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "Using cached frozendict-2.4.6-py311-none-any.whl (16 kB)\n",
      "Using cached frozenlist-1.7.0-cp311-cp311-macosx_11_0_arm64.whl (47 kB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading kiwisolver-1.4.9-cp311-cp311-macosx_11_0_arm64.whl (65 kB)\n",
      "Downloading narwhals-2.1.1-py3-none-any.whl (389 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached propcache-0.3.2-cp311-cp311-macosx_11_0_arm64.whl (43 kB)\n",
      "Using cached protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl (425 kB)\n",
      "Using cached pyarrow-21.0.0-cp311-cp311-macosx_12_0_arm64.whl (31.2 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Using cached regex-2025.7.34-cp311-cp311-macosx_11_0_arm64.whl (285 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Using cached scipy-1.16.1-cp311-cp311-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached websockets-15.0.1-cp311-cp311-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: pytz, peewee, multitasking, xxhash, websockets, urllib3, tzdata, tqdm, threadpoolctl, soupsieve, safetensors, regex, pyyaml, pyparsing, pycparser, pyarrow, protobuf, propcache, pillow, numpy, narwhals, multidict, kiwisolver, joblib, idna, hf-xet, fsspec, frozenlist, frozendict, fonttools, filelock, dill, cycler, click, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, scipy, requests, plotly, pandas, nltk, multiprocess, contourpy, cffi, beautifulsoup4, aiosignal, textblob, scikit-learn, matplotlib, huggingface-hub, curl_cffi, aiohttp, yfinance, tokenizers, seaborn, transformers, datasets\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60/60\u001b[0m [datasets]datasets]transformers]ub]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 beautifulsoup4-4.13.4 certifi-2025.8.3 cffi-1.17.1 charset_normalizer-3.4.3 click-8.2.1 contourpy-1.3.3 curl_cffi-0.13.0 cycler-0.12.1 datasets-4.0.0 dill-0.3.8 filelock-3.18.0 fonttools-4.59.0 frozendict-2.4.6 frozenlist-1.7.0 fsspec-2025.3.0 hf-xet-1.1.7 huggingface-hub-0.34.4 idna-3.10 joblib-1.5.1 kiwisolver-1.4.9 matplotlib-3.10.5 multidict-6.6.4 multiprocess-0.70.16 multitasking-0.0.12 narwhals-2.1.1 nltk-3.9.1 numpy-2.3.2 pandas-2.3.1 peewee-3.18.2 pillow-11.3.0 plotly-6.3.0 propcache-0.3.2 protobuf-6.31.1 pyarrow-21.0.0 pycparser-2.22 pyparsing-3.2.3 pytz-2025.2 pyyaml-6.0.2 regex-2025.7.34 requests-2.32.4 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 seaborn-0.13.2 soupsieve-2.7 textblob-0.19.0 threadpoolctl-3.6.0 tokenizers-0.21.4 tqdm-4.67.1 transformers-4.55.1 tzdata-2025.2 urllib3-2.5.0 websockets-15.0.1 xxhash-3.5.0 yarl-1.20.1 yfinance-0.2.65\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## installation\n",
    "%pip install pandas numpy scikit-learn matplotlib seaborn plotly yfinance textblob requests datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edcdd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import yfinance as yf\n",
    "from textblob import TextBlob\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score, \n",
    "                           mean_absolute_percentage_error, explained_variance_score)\n",
    "\n",
    "# Deep Learning (optional)\n",
    "try:\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    NEURAL_NETWORK_AVAILABLE = True\n",
    "except ImportError:\n",
    "    NEURAL_NETWORK_AVAILABLE = False\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.dates import DateFormatter\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Hugging Face Integration\n",
    "try:\n",
    "    from datasets import load_dataset, load_dataset_builder\n",
    "    from transformers import pipeline\n",
    "    HUGGINGFACE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    HUGGINGFACE_AVAILABLE = False\n",
    "    print(\"HuggingFace libraries not installed. Using basic sentiment analysis.\")\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05970cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "\n",
    "class AdvancedStockPredictor:\n",
    "    def __init__(self, news_api_key=None, use_advanced_sentiment=True):\n",
    "        \"\"\"\n",
    "        Advanced Stock Prediction Pipeline with comprehensive evaluation\n",
    "        \n",
    "        Args:\n",
    "            news_api_key (str): API key for news data\n",
    "            use_advanced_sentiment (bool): Use FinBERT for sentiment analysis\n",
    "        \"\"\"\n",
    "        self.news_api_key = news_api_key\n",
    "        self.use_advanced_sentiment = use_advanced_sentiment\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.feature_columns = []\n",
    "        self.training_history = {}\n",
    "        self.evaluation_results = {}\n",
    "        \n",
    "        # Initialize sentiment analyzer\n",
    "        self.setup_sentiment_analyzer()\n",
    "        \n",
    "    def setup_sentiment_analyzer(self):\n",
    "        \"\"\"Setup sentiment analysis pipeline\"\"\"\n",
    "        if HUGGINGFACE_AVAILABLE and self.use_advanced_sentiment:\n",
    "            try:\n",
    "                # Use FinBERT for financial sentiment analysis\n",
    "                self.sentiment_analyzer = pipeline(\n",
    "                    \"sentiment-analysis\",\n",
    "                    model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\",\n",
    "                    tokenizer=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "                )\n",
    "                self.advanced_sentiment = True\n",
    "                print(\"âœ“ Advanced FinBERT sentiment analyzer loaded\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load advanced sentiment analyzer: {e}\")\n",
    "                self.advanced_sentiment = False\n",
    "        else:\n",
    "            self.advanced_sentiment = False\n",
    "            print(\"âœ“ Using TextBlob for sentiment analysis\")\n",
    "    \n",
    "    def load_huggingface_datasets(self):\n",
    "        \"\"\"\n",
    "        Load financial datasets from Hugging Face\n",
    "        \n",
    "        Returns:\n",
    "            dict: Loaded datasets\n",
    "        \"\"\"\n",
    "        datasets_info = {}\n",
    "        \n",
    "        if not HUGGINGFACE_AVAILABLE:\n",
    "            print(\"HuggingFace datasets not available. Please install: pip install datasets transformers\")\n",
    "            return datasets_info\n",
    "\n",
    "        # Define datasets to load and their splits/samples\n",
    "        datasets_to_load = {\n",
    "            \"Zihan1004/FNSPID\": \"train[:1000]\",\n",
    "            \"ashraq/financial-news\": \"train[:5000]\",\n",
    "            \"takala/financial_phrasebank\": \"sentences_allagree[:2000]\"\n",
    "        }\n",
    "\n",
    "        for dataset_name, split_info in datasets_to_load.items():\n",
    "            try:\n",
    "                print(f\"Loading {dataset_name} financial dataset...\")\n",
    "\n",
    "                # Check if dataset is already downloaded\n",
    "                builder = load_dataset_builder(dataset_name)\n",
    "                dataset_cache_dir = builder.dataset_info.builder_name\n",
    "                cache_path = os.path.join(builder.cache_dir, dataset_cache_dir)\n",
    "\n",
    "                if os.path.exists(cache_path):\n",
    "                    print(f\"Dataset {dataset_name} found in cache. Loading from cache...\")\n",
    "                    # Load the dataset from cache\n",
    "                    dataset = load_dataset(dataset_name, split=split_info)\n",
    "                    datasets_info[dataset_name.split('/')[-1].lower()] = dataset\n",
    "                    print(f\"âœ“ Loaded {dataset_name} dataset with {len(dataset)} records from cache\")\n",
    "                else:\n",
    "                    print(f\"Dataset {dataset_name} not found in cache. Downloading...\")\n",
    "                    # Download and load the dataset\n",
    "                    dataset = load_dataset(dataset_name, split=split_info)\n",
    "                    datasets_info[dataset_name.split('/')[-1].lower()] = dataset\n",
    "                    print(f\"âœ“ Downloaded and loaded {dataset_name} dataset with {len(dataset)} records\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load {dataset_name} dataset: {e}\")\n",
    "\n",
    "        \n",
    "        # try:\n",
    "        #     # Load FNSPID dataset (if available)\n",
    "        #     print(\"Loading FNSPID financial dataset...\")\n",
    "        #     fnspid_dataset = load_dataset(\"Zihan1004/FNSPID\", split=\"train[:1000]\")  # Load sample\n",
    "        #     datasets_info['fnspid'] = fnspid_dataset\n",
    "        #     print(f\"âœ“ Loaded FNSPID dataset with {len(fnspid_dataset)} records\")\n",
    "            \n",
    "        # except Exception as e:\n",
    "        #     print(f\"Could not load FNSPID dataset: {e}\")\n",
    "        \n",
    "        # try:\n",
    "        #     # Load financial news dataset\n",
    "        #     print(\"Loading financial news dataset...\")\n",
    "        #     news_dataset = load_dataset(\"ashraq/financial-news\", split=\"train[:5000]\")\n",
    "        #     datasets_info['financial_news'] = news_dataset\n",
    "        #     print(f\"âœ“ Loaded financial news dataset with {len(news_dataset)} records\")\n",
    "            \n",
    "        # except Exception as e:\n",
    "        #     print(f\"Could not load financial news dataset: {e}\")\n",
    "        \n",
    "        # try:\n",
    "        #     # Load financial sentiment dataset\n",
    "        #     print(\"Loading financial sentiment dataset...\")\n",
    "        #     sentiment_dataset = load_dataset(\"takala/financial_phrasebank\", \n",
    "        #                                    \"sentences_allagree\", split=\"train[:2000]\")\n",
    "        #     datasets_info['sentiment'] = sentiment_dataset\n",
    "        #     print(f\"âœ“ Loaded sentiment dataset with {len(sentiment_dataset)} records\")\n",
    "            \n",
    "        # except Exception as e:\n",
    "        #     print(f\"Could not load sentiment dataset: {e}\")\n",
    "        \n",
    "        return datasets_info\n",
    "    \n",
    "    def fetch_stock_data(self, symbol, period=\"2y\", interval=\"1d\"):\n",
    "        \"\"\"Enhanced stock data fetching with error handling\"\"\"\n",
    "        try:\n",
    "            stock = yf.Ticker(symbol)\n",
    "            data = stock.history(period=period, interval=interval)\n",
    "            \n",
    "            if data.empty:\n",
    "                print(f\"No data found for symbol {symbol}\")\n",
    "                return None\n",
    "                \n",
    "            data.reset_index(inplace=True)\n",
    "            \n",
    "            # Add company info\n",
    "            try:\n",
    "                info = stock.info\n",
    "                company_name = info.get('longName', symbol)\n",
    "                sector = info.get('sector', 'Unknown')\n",
    "                print(f\"âœ“ Fetched data for {company_name} ({symbol}) - Sector: {sector}\")\n",
    "            except:\n",
    "                print(f\"âœ“ Fetched data for {symbol}\")\n",
    "                \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching stock data for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def advanced_sentiment_analysis(self, headlines):\n",
    "        \"\"\"\n",
    "        Advanced sentiment analysis using FinBERT or TextBlob\n",
    "        \n",
    "        Args:\n",
    "            headlines (list): List of news headlines\n",
    "            \n",
    "        Returns:\n",
    "            dict: Comprehensive sentiment analysis results\n",
    "        \"\"\"\n",
    "        if not headlines:\n",
    "            return self.get_default_sentiment()\n",
    "        \n",
    "        sentiments = []\n",
    "        \n",
    "        if self.advanced_sentiment:\n",
    "            # Use FinBERT for financial sentiment\n",
    "            for headline in headlines:\n",
    "                try:\n",
    "                    result = self.sentiment_analyzer(headline)[0]\n",
    "                    \n",
    "                    # Convert to numeric scores\n",
    "                    if result['label'] == 'POSITIVE':\n",
    "                        polarity = result['score']\n",
    "                    elif result['label'] == 'NEGATIVE':\n",
    "                        polarity = -result['score']\n",
    "                    else:  # NEUTRAL\n",
    "                        polarity = 0\n",
    "                    \n",
    "                    sentiments.append({\n",
    "                        'headline': headline,\n",
    "                        'polarity': polarity,\n",
    "                        'confidence': result['score'],\n",
    "                        'label': result['label']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    # Fallback to TextBlob\n",
    "                    blob = TextBlob(headline)\n",
    "                    sentiments.append({\n",
    "                        'headline': headline,\n",
    "                        'polarity': blob.sentiment.polarity,\n",
    "                        'confidence': 0.5,\n",
    "                        'label': 'NEUTRAL'\n",
    "                    })\n",
    "        else:\n",
    "            # Use TextBlob\n",
    "            for headline in headlines:\n",
    "                blob = TextBlob(headline)\n",
    "                polarity = blob.sentiment.polarity\n",
    "                \n",
    "                if polarity > 0.1:\n",
    "                    label = 'POSITIVE'\n",
    "                elif polarity < -0.1:\n",
    "                    label = 'NEGATIVE'\n",
    "                else:\n",
    "                    label = 'NEUTRAL'\n",
    "                    \n",
    "                sentiments.append({\n",
    "                    'headline': headline,\n",
    "                    'polarity': polarity,\n",
    "                    'confidence': abs(polarity),\n",
    "                    'label': label\n",
    "                })\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        polarities = [s['polarity'] for s in sentiments]\n",
    "        confidences = [s['confidence'] for s in sentiments]\n",
    "        \n",
    "        return {\n",
    "            'individual_sentiments': sentiments,\n",
    "            'avg_polarity': np.mean(polarities),\n",
    "            'polarity_std': np.std(polarities),\n",
    "            'avg_confidence': np.mean(confidences),\n",
    "            'positive_count': sum(1 for s in sentiments if s['label'] == 'POSITIVE'),\n",
    "            'negative_count': sum(1 for s in sentiments if s['label'] == 'NEGATIVE'),\n",
    "            'neutral_count': sum(1 for s in sentiments if s['label'] == 'NEUTRAL'),\n",
    "            'sentiment_momentum': np.mean(polarities[-3:]) if len(polarities) >= 3 else np.mean(polarities),\n",
    "            'volatility': np.std(polarities) if len(polarities) > 1 else 0\n",
    "        }\n",
    "    \n",
    "    def get_default_sentiment(self):\n",
    "        \"\"\"Default sentiment when no news available\"\"\"\n",
    "        return {\n",
    "            'individual_sentiments': [],\n",
    "            'avg_polarity': 0,\n",
    "            'polarity_std': 0,\n",
    "            'avg_confidence': 0,\n",
    "            'positive_count': 0,\n",
    "            'negative_count': 0,\n",
    "            'neutral_count': 0,\n",
    "            'sentiment_momentum': 0,\n",
    "            'volatility': 0\n",
    "        }\n",
    "    \n",
    "    def create_advanced_technical_indicators(self, df):\n",
    "        \"\"\"Enhanced technical indicators\"\"\"\n",
    "        # Existing indicators\n",
    "        df = self.create_basic_technical_indicators(df)\n",
    "        \n",
    "        # Advanced indicators\n",
    "        # Stochastic Oscillator\n",
    "        low_min = df['Low'].rolling(window=14).min()\n",
    "        high_max = df['High'].rolling(window=14).max()\n",
    "        df['Stochastic_%K'] = 100 * ((df['Close'] - low_min) / (high_max - low_min))\n",
    "        df['Stochastic_%D'] = df['Stochastic_%K'].rolling(window=3).mean()\n",
    "        \n",
    "        # Williams %R\n",
    "        df['Williams_%R'] = -100 * ((high_max - df['Close']) / (high_max - low_min))\n",
    "        \n",
    "        # Average True Range (ATR)\n",
    "        high_low = df['High'] - df['Low']\n",
    "        high_close = np.abs(df['High'] - df['Close'].shift())\n",
    "        low_close = np.abs(df['Low'] - df['Close'].shift())\n",
    "        true_range = np.maximum(high_low, np.maximum(high_close, low_close))\n",
    "        df['ATR'] = true_range.rolling(window=14).mean()\n",
    "        \n",
    "        # Commodity Channel Index (CCI)\n",
    "        tp = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "        df['CCI'] = (tp - tp.rolling(window=20).mean()) / (0.015 * tp.rolling(window=20).std())\n",
    "        \n",
    "        # On-Balance Volume (OBV)\n",
    "        df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).fillna(0).cumsum()\n",
    "        \n",
    "        # Price Rate of Change\n",
    "        df['ROC'] = df['Close'].pct_change(periods=12) * 100\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_basic_technical_indicators(self, df):\n",
    "        \"\"\"Basic technical indicators from original model\"\"\"\n",
    "        # Moving Averages\n",
    "        for window in [5, 10, 20, 50]:\n",
    "            df[f'SMA_{window}'] = df['Close'].rolling(window=window).mean()\n",
    "            \n",
    "        # Exponential Moving Averages\n",
    "        df['EMA_12'] = df['Close'].ewm(span=12).mean()\n",
    "        df['EMA_26'] = df['Close'].ewm(span=26).mean()\n",
    "        \n",
    "        # MACD\n",
    "        df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "        df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
    "        df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
    "        \n",
    "        # RSI\n",
    "        delta = df['Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "        rs = gain / loss\n",
    "        df['RSI'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        df['BB_Middle'] = df['Close'].rolling(window=20).mean()\n",
    "        bb_std = df['Close'].rolling(window=20).std()\n",
    "        df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)\n",
    "        df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)\n",
    "        df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / df['BB_Middle']\n",
    "        df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "        \n",
    "        # Volume indicators\n",
    "        df['Volume_SMA'] = df['Volume'].rolling(window=10).mean()\n",
    "        df['Volume_Ratio'] = df['Volume'] / df['Volume_SMA']\n",
    "        \n",
    "        # Price change indicators\n",
    "        df['Price_Change'] = df['Close'].pct_change()\n",
    "        df['Price_Change_5d'] = df['Close'].pct_change(periods=5)\n",
    "        df['Price_Volatility'] = df['Price_Change'].rolling(window=20).std()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_comprehensive_training_data(self, stock_data, sentiment_data=None, target_days=1):\n",
    "        \"\"\"\n",
    "        Prepare comprehensive training data with multiple prediction targets\n",
    "        \n",
    "        Args:\n",
    "            stock_data (pd.DataFrame): Historical stock data\n",
    "            sentiment_data (dict): Sentiment analysis results\n",
    "            target_days (int): Number of days ahead to predict (1, 3, 5, 7)\n",
    "        \"\"\"\n",
    "        # Create technical indicators\n",
    "        stock_data = self.create_advanced_technical_indicators(stock_data)\n",
    "        \n",
    "        # Add sentiment features\n",
    "        if sentiment_data:\n",
    "            for key, value in sentiment_data.items():\n",
    "                if key != 'individual_sentiments':\n",
    "                    stock_data[f'sentiment_{key}'] = value\n",
    "        else:\n",
    "            # Default sentiment features\n",
    "            default_sentiment = self.get_default_sentiment()\n",
    "            for key, value in default_sentiment.items():\n",
    "                if key != 'individual_sentiments':\n",
    "                    stock_data[f'sentiment_{key}'] = value\n",
    "        \n",
    "        # Create target variables for different time horizons\n",
    "        stock_data[f'Target_{target_days}d'] = stock_data['Close'].shift(-target_days)\n",
    "        stock_data[f'Target_{target_days}d_Change'] = (\n",
    "            (stock_data[f'Target_{target_days}d'] - stock_data['Close']) / stock_data['Close']\n",
    "        ) * 100\n",
    "        \n",
    "        # Feature engineering\n",
    "        # Lagged features\n",
    "        for lag in [1, 2, 3, 5]:\n",
    "            stock_data[f'Close_lag_{lag}'] = stock_data['Close'].shift(lag)\n",
    "            stock_data[f'Volume_lag_{lag}'] = stock_data['Volume'].shift(lag)\n",
    "            stock_data[f'RSI_lag_{lag}'] = stock_data['RSI'].shift(lag)\n",
    "        \n",
    "        # Rolling statistics\n",
    "        for window in [5, 10, 20]:\n",
    "            stock_data[f'Close_std_{window}'] = stock_data['Close'].rolling(window).std()\n",
    "            stock_data[f'Volume_std_{window}'] = stock_data['Volume'].rolling(window).std()\n",
    "        \n",
    "        # Define feature columns\n",
    "        technical_features = [\n",
    "            'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "            'SMA_5', 'SMA_10', 'SMA_20', 'SMA_50',\n",
    "            'EMA_12', 'EMA_26', 'MACD', 'MACD_Signal', 'MACD_Histogram',\n",
    "            'RSI', 'BB_Upper', 'BB_Lower', 'BB_Width', 'BB_Position',\n",
    "            'Volume_Ratio', 'Price_Change', 'Price_Change_5d', 'Price_Volatility',\n",
    "            'Stochastic_%K', 'Stochastic_%D', 'Williams_%R', 'ATR', 'CCI', 'OBV', 'ROC'\n",
    "        ]\n",
    "        \n",
    "        sentiment_features = [\n",
    "            'sentiment_avg_polarity', 'sentiment_polarity_std', 'sentiment_avg_confidence',\n",
    "            'sentiment_positive_count', 'sentiment_negative_count', 'sentiment_neutral_count',\n",
    "            'sentiment_sentiment_momentum', 'sentiment_volatility'\n",
    "        ]\n",
    "        \n",
    "        lag_features = [f'Close_lag_{lag}' for lag in [1, 2, 3, 5]]\n",
    "        lag_features += [f'Volume_lag_{lag}' for lag in [1, 2, 3, 5]]\n",
    "        lag_features += [f'RSI_lag_{lag}' for lag in [1, 2, 3, 5]]\n",
    "        \n",
    "        rolling_features = []\n",
    "        for window in [5, 10, 20]:\n",
    "            rolling_features += [f'Close_std_{window}', f'Volume_std_{window}']\n",
    "        \n",
    "        self.feature_columns = technical_features + sentiment_features + lag_features + rolling_features\n",
    "        \n",
    "        # Remove rows with NaN values\n",
    "        stock_data = stock_data.dropna()\n",
    "        \n",
    "        return stock_data\n",
    "    \n",
    "    def train_ensemble_models(self, training_data, target_days=1, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Train ensemble of models with comprehensive evaluation\n",
    "        \n",
    "        Args:\n",
    "            training_data (pd.DataFrame): Prepared training data\n",
    "            target_days (int): Prediction horizon\n",
    "            test_size (float): Test set size\n",
    "            \n",
    "        Returns:\n",
    "            dict: Training results and evaluation metrics\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸš€ Training ensemble models for {target_days}-day prediction...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        target_col = f'Target_{target_days}d'\n",
    "        X = training_data[self.feature_columns]\n",
    "        y = training_data[target_col]\n",
    "        \n",
    "        print(f\"Training data shape: {X.shape}\")\n",
    "        print(f\"Target variable: {target_col}\")\n",
    "        \n",
    "        # Time series split for proper validation\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        \n",
    "        # Regular train-test split (keeping temporal order)\n",
    "        split_idx = int(len(X) * (1 - test_size))\n",
    "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "        \n",
    "        print(f\"Training set: {len(X_train)} samples\")\n",
    "        print(f\"Test set: {len(X_test)} samples\")\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Define models\n",
    "        models = {\n",
    "            'Random Forest': RandomForestRegressor(\n",
    "                n_estimators=200, max_depth=10, min_samples_split=5,\n",
    "                min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(\n",
    "                n_estimators=200, learning_rate=0.1, max_depth=6,\n",
    "                min_samples_split=5, random_state=42\n",
    "            ),\n",
    "            'Linear Regression': LinearRegression()\n",
    "        }\n",
    "        \n",
    "        if NEURAL_NETWORK_AVAILABLE:\n",
    "            models['Neural Network'] = MLPRegressor(\n",
    "                hidden_layer_sizes=(100, 50, 25), max_iter=1000,\n",
    "                random_state=42, early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Train and evaluate models\n",
    "        model_results = {}\n",
    "        trained_models = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nðŸ“Š Training {name}...\")\n",
    "            \n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(model, X_train_scaled, y_train, \n",
    "                                      cv=tscv, scoring='r2', n_jobs=-1)\n",
    "            \n",
    "            # Train on full training set\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_train_pred = model.predict(X_train_scaled)\n",
    "            y_test_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            train_metrics = self.calculate_metrics(y_train, y_train_pred)\n",
    "            test_metrics = self.calculate_metrics(y_test, y_test_pred)\n",
    "            \n",
    "            model_results[name] = {\n",
    "                'cv_scores': cv_scores,\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std(),\n",
    "                'train_metrics': train_metrics,\n",
    "                'test_metrics': test_metrics,\n",
    "                'predictions': {\n",
    "                    'y_train': y_train,\n",
    "                    'y_train_pred': y_train_pred,\n",
    "                    'y_test': y_test,\n",
    "                    'y_test_pred': y_test_pred\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            trained_models[name] = model\n",
    "            \n",
    "            print(f\"  CV RÂ² Score: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "            print(f\"  Test RÂ² Score: {test_metrics['r2']:.4f}\")\n",
    "            print(f\"  Test MAE: ${test_metrics['mae']:.2f}\")\n",
    "        \n",
    "        # Create ensemble model\n",
    "        print(f\"\\nðŸŽ¯ Creating ensemble model...\")\n",
    "        ensemble = VotingRegressor([\n",
    "            (name, model) for name, model in trained_models.items()\n",
    "        ])\n",
    "        ensemble.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Ensemble predictions\n",
    "        y_train_pred_ensemble = ensemble.predict(X_train_scaled)\n",
    "        y_test_pred_ensemble = ensemble.predict(X_test_scaled)\n",
    "        \n",
    "        # Ensemble metrics\n",
    "        train_metrics_ensemble = self.calculate_metrics(y_train, y_train_pred_ensemble)\n",
    "        test_metrics_ensemble = self.calculate_metrics(y_test, y_test_pred_ensemble)\n",
    "        \n",
    "        model_results['Ensemble'] = {\n",
    "            'train_metrics': train_metrics_ensemble,\n",
    "            'test_metrics': test_metrics_ensemble,\n",
    "            'predictions': {\n",
    "                'y_train': y_train,\n",
    "                'y_train_pred': y_train_pred_ensemble,\n",
    "                'y_test': y_test,\n",
    "                'y_test_pred': y_test_pred_ensemble\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        trained_models['Ensemble'] = ensemble\n",
    "        \n",
    "        print(f\"  Ensemble Test RÂ² Score: {test_metrics_ensemble['r2']:.4f}\")\n",
    "        print(f\"  Ensemble Test MAE: ${test_metrics_ensemble['mae']:.2f}\")\n",
    "        \n",
    "        # Store models and scalers\n",
    "        self.models[f'{target_days}d'] = trained_models\n",
    "        self.scalers[f'{target_days}d'] = scaler\n",
    "        \n",
    "        # Feature importance (from Random Forest)\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': self.feature_columns,\n",
    "            'importance': trained_models['Random Forest'].feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return {\n",
    "            'model_results': model_results,\n",
    "            'feature_importance': feature_importance,\n",
    "            'data_info': {\n",
    "                'train_size': len(X_train),\n",
    "                'test_size': len(X_test),\n",
    "                'n_features': len(self.feature_columns),\n",
    "                'target_days': target_days\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "        return {\n",
    "            'mae': mean_absolute_error(y_true, y_pred),\n",
    "            'mse': mean_squared_error(y_true, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'r2': r2_score(y_true, y_pred),\n",
    "            'mape': mean_absolute_percentage_error(y_true, y_pred) * 100,\n",
    "            'explained_variance': explained_variance_score(y_true, y_pred)\n",
    "        }\n",
    "    \n",
    "    def create_evaluation_visualizations(self, training_results, symbol=\"Stock\"):\n",
    "        \"\"\"\n",
    "        Create comprehensive evaluation visualizations\n",
    "        \n",
    "        Args:\n",
    "            training_results (dict): Results from model training\n",
    "            symbol (str): Stock symbol for titles\n",
    "        \"\"\"\n",
    "        model_results = training_results['model_results']\n",
    "        feature_importance = training_results['feature_importance']\n",
    "        \n",
    "        # Set up the plotting style\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        fig = plt.figure(figsize=(20, 24))\n",
    "        \n",
    "        # 1. Model Performance Comparison\n",
    "        ax1 = plt.subplot(4, 3, 1)\n",
    "        models = list(model_results.keys())\n",
    "        r2_scores = [model_results[model]['test_metrics']['r2'] for model in models]\n",
    "        mae_scores = [model_results[model]['test_metrics']['mae'] for model in models]\n",
    "        \n",
    "        x = np.arange(len(models))\n",
    "        ax1.bar(x, r2_scores, alpha=0.7, color='skyblue')\n",
    "        ax1.set_xlabel('Models')\n",
    "        ax1.set_ylabel('RÂ² Score')\n",
    "        ax1.set_title(f'{symbol} - Model RÂ² Comparison')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(models, rotation=45)\n",
    "        for i, v in enumerate(r2_scores):\n",
    "            ax1.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        # 2. MAE Comparison\n",
    "        ax2 = plt.subplot(4, 3, 2)\n",
    "        ax2.bar(x, mae_scores, alpha=0.7, color='lightcoral')\n",
    "        ax2.set_xlabel('Models')\n",
    "        ax2.set_ylabel('Mean Absolute Error ($)')\n",
    "        ax2.set_title(f'{symbol} - Model MAE Comparison')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(models, rotation=45)\n",
    "        for i, v in enumerate(mae_scores):\n",
    "            ax2.text(i, v + max(mae_scores)*0.02, f'${v:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        # 3. Feature Importance (Top 15)\n",
    "        ax3 = plt.subplot(4, 3, 3)\n",
    "        top_features = feature_importance.head(15)\n",
    "        ax3.barh(range(len(top_features)), top_features['importance'], alpha=0.7, color='lightgreen')\n",
    "        ax3.set_yticks(range(len(top_features)))\n",
    "        ax3.set_yticklabels(top_features['feature'])\n",
    "        ax3.set_xlabel('Importance')\n",
    "        ax3.set_title(f'{symbol} - Top 15 Feature Importance')\n",
    "        ax3.invert_yaxis()\n",
    "        \n",
    "        # 4. Prediction vs Actual (Best Model)\n",
    "        best_model = max(models, key=lambda x: model_results[x]['test_metrics']['r2'])\n",
    "        predictions = model_results[best_model]['predictions']\n",
    "        \n",
    "        ax4 = plt.subplot(4, 3, 4)\n",
    "        ax4.scatter(predictions['y_test'], predictions['y_test_pred'], alpha=0.6, color='purple')\n",
    "        min_val = min(predictions['y_test'].min(), predictions['y_test_pred'].min())\n",
    "        max_val = max(predictions['y_test'].max(), predictions['y_test_pred'].max())\n",
    "        ax4.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "        ax4.set_xlabel('Actual Price ($)')\n",
    "        ax4.set_ylabel('Predicted Price ($)')\n",
    "        ax4.set_title(f'{symbol} - {best_model} Predictions vs Actual')\n",
    "        \n",
    "        # Add RÂ² to the plot\n",
    "        r2 = model_results[best_model]['test_metrics']['r2']\n",
    "        ax4.text(0.05, 0.95, f'RÂ² = {r2:.4f}', transform=ax4.transAxes, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # 5. Residuals Plot\n",
    "        ax5 = plt.subplot(4, 3, 5)\n",
    "        residuals = predictions['y_test'] - predictions['y_test_pred']\n",
    "        ax5.scatter(predictions['y_test_pred'], residuals, alpha=0.6, color='orange')\n",
    "        ax5.axhline(y=0, color='r', linestyle='--')\n",
    "        ax5.set_xlabel('Predicted Price ($)')\n",
    "        ax5.set_ylabel('Residuals ($)')\n",
    "        ax5.set_title(f'{symbol} - {best_model} Residuals Plot')\n",
    "        \n",
    "        # 6. Time Series of Predictions (last 100 points)\n",
    "        ax6 = plt.subplot(4, 3, 6)\n",
    "        last_n = min(100, len(predictions['y_test']))\n",
    "        indices = range(last_n)\n",
    "        ax6.plot(indices, predictions['y_test'].iloc[-last_n:], 'b-', label='Actual', linewidth=2)\n",
    "        ax6.plot(indices, predictions['y_test_pred'][-last_n:], 'r--', label='Predicted', linewidth=2)\n",
    "        ax6.set_xlabel('Time')\n",
    "        ax6.set_ylabel('Price ($)')\n",
    "        ax6.set_title(f'{symbol} - Time Series Comparison (Last {last_n} points)')\n",
    "        ax6.legend()\n",
    "        \n",
    "        # 7. Error Distribution\n",
    "        ax7 = plt.subplot(4, 3, 7)\n",
    "        ax7.hist(residuals, bins=30, alpha=0.7, color='teal')\n",
    "        ax7.set_xlabel('Residuals ($)')\n",
    "        ax7.set_ylabel('Frequency')\n",
    "        ax7.set_title(f'{symbol} - Error Distribution')\n",
    "        ax7.axvline(x=0, color='r', linestyle='--')\n",
    "        \n",
    "        # 8. Cross-validation scores (if available)\n",
    "        ax8 = plt.subplot(4, 3, 8)\n",
    "        cv_models = [name for name in models if 'cv_scores' in model_results[name]]\n",
    "        if cv_models:\n",
    "            cv_data = [model_results[name]['cv_scores'] for name in cv_models]\n",
    "            ax8.boxplot(cv_data, labels=cv_models)\n",
    "            ax8.set_ylabel('RÂ² Score')\n",
    "            ax8.set_title(f'{symbol} - Cross-Validation Scores')\n",
    "            ax8.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 9. Learning Curve (simplified)\n",
    "        ax9 = plt.subplot(4, 3, 9)\n",
    "        train_sizes = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "        train_r2 = [model_results[best_model]['train_metrics']['r2'] * (0.95 + 0.05 * size) for size in train_sizes]\n",
    "        test_r2 = [model_results[best_model]['test_metrics']['r2'] * (0.85 + 0.15 * size) for size in train_sizes]\n",
    "        \n",
    "        ax9.plot(train_sizes, train_r2, 'o-', label='Training Score', color='blue')\n",
    "        ax9.plot(train_sizes, test_r2, 'o-', label='Validation Score', color='red')\n",
    "        ax9.set_xlabel('Training Set Size')\n",
    "        ax9.set_ylabel('RÂ² Score')\n",
    "        ax9.set_title(f'{symbol} - Learning Curve (Simulated)')\n",
    "        ax9.legend()\n",
    "        ax9.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 10. Metrics Comparison Heatmap\n",
    "        ax10 = plt.subplot(4, 3, 10)\n",
    "        metrics_data = []\n",
    "        metrics_names = ['r2', 'mae', 'mse', 'mape']\n",
    "        \n",
    "        for model in models:\n",
    "            row = []\n",
    "            for metric in metrics_names:\n",
    "                value = model_results[model]['test_metrics'][metric]\n",
    "                # Normalize for visualization\n",
    "                if metric == 'r2':\n",
    "                    row.append(value)\n",
    "                else:\n",
    "                    # For error metrics, use inverse for better visualization\n",
    "                    row.append(1 / (1 + value))\n",
    "            metrics_data.append(row)\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics_data, index=models, columns=metrics_names)\n",
    "        sns.heatmap(metrics_df, annot=True, cmap='RdYlGn', ax=ax10, fmt='.3f')\n",
    "        ax10.set_title(f'{symbol} - Model Metrics Heatmap')\n",
    "        \n",
    "        # 11. Prediction Accuracy by Price Range\n",
    "        ax11 = plt.subplot(4, 3, 11)\n",
    "        price_ranges = pd.cut(predictions['y_test'], bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "        accuracy_by_range = []\n",
    "        \n",
    "        for price_range in price_ranges.cat.categories:\n",
    "            mask = price_ranges == price_range\n",
    "            if mask.sum() > 0:\n",
    "                range_mae = mean_absolute_error(\n",
    "                    predictions['y_test'][mask], \n",
    "                    predictions['y_test_pred'][mask]\n",
    "                )\n",
    "                accuracy_by_range.append(range_mae)\n",
    "            else:\n",
    "                accuracy_by_range.append(0)\n",
    "        \n",
    "        ax11.bar(price_ranges.cat.categories, accuracy_by_range, alpha=0.7, color='gold')\n",
    "        ax11.set_xlabel('Price Range')\n",
    "        ax11.set_ylabel('MAE ($)')\n",
    "        ax11.set_title(f'{symbol} - Prediction Accuracy by Price Range')\n",
    "        ax11.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 12. Model Confidence Analysis\n",
    "        ax12 = plt.subplot(4, 3, 12)\n",
    "        prediction_errors = np.abs(residuals)\n",
    "        confidence_score = 1 - (prediction_errors / predictions['y_test'])\n",
    "        confidence_score = np.clip(confidence_score, 0, 1)  # Clip between 0 and 1\n",
    "        \n",
    "        ax12.hist(confidence_score, bins=20, alpha=0.7, color='mediumpurple')\n",
    "        ax12.set_xlabel('Confidence Score')\n",
    "        ax12.set_ylabel('Frequency')\n",
    "        ax12.set_title(f'{symbol} - Prediction Confidence Distribution')\n",
    "        ax12.axvline(x=confidence_score.mean(), color='r', linestyle='--', \n",
    "                    label=f'Mean: {confidence_score.mean():.3f}')\n",
    "        ax12.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\nðŸ“ˆ EVALUATION SUMMARY FOR {symbol}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Best Model: {best_model}\")\n",
    "        print(f\"Best RÂ² Score: {model_results[best_model]['test_metrics']['r2']:.4f}\")\n",
    "        print(f\"Best MAE: ${model_results[best_model]['test_metrics']['mae']:.2f}\")\n",
    "        print(f\"Mean Confidence Score: {confidence_score.mean():.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'best_model': best_model,\n",
    "            'confidence_scores': confidence_score,\n",
    "            'residuals': residuals\n",
    "        }\n",
    "    \n",
    "    def fetch_recent_news(self, symbol, company_name, days_back=7):\n",
    "        \"\"\"\n",
    "        Fetch recent news for prediction\n",
    "        \"\"\"\n",
    "        # Enhanced news fetching with multiple sources\n",
    "        headlines = []\n",
    "        \n",
    "        if self.news_api_key:\n",
    "            try:\n",
    "                # Use real news API\n",
    "                url = \"https://newsapi.org/v2/everything\"\n",
    "                end_date = datetime.now()\n",
    "                start_date = end_date - timedelta(days=days_back)\n",
    "                \n",
    "                params = {\n",
    "                    'q': f'\"{symbol}\" OR \"{company_name}\"',\n",
    "                    'from': start_date.strftime('%Y-%m-%d'),\n",
    "                    'to': end_date.strftime('%Y-%m-%d'),\n",
    "                    'sortBy': 'relevancy',\n",
    "                    'language': 'en',\n",
    "                    'apiKey': self.news_api_key,\n",
    "                    'pageSize': 50\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, params=params)\n",
    "                data = response.json()\n",
    "                \n",
    "                if data['status'] == 'ok':\n",
    "                    headlines = [article['title'] for article in data['articles'][:20]]\n",
    "                    print(f\"âœ“ Fetched {len(headlines)} recent news headlines\")\n",
    "                else:\n",
    "                    print(f\"News API error: {data.get('message')}\")\n",
    "                    headlines = self.get_mock_headlines(symbol)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching news: {e}\")\n",
    "                headlines = self.get_mock_headlines(symbol)\n",
    "        else:\n",
    "            headlines = self.get_mock_headlines(symbol)\n",
    "        \n",
    "        return headlines\n",
    "    \n",
    "    def get_mock_headlines(self, symbol):\n",
    "        \"\"\"Generate mock headlines for demonstration\"\"\"\n",
    "        positive_templates = [\n",
    "            f\"{symbol} reports strong quarterly earnings\",\n",
    "            f\"{symbol} announces breakthrough innovation\",\n",
    "            f\"{symbol} stock reaches new highs on positive outlook\",\n",
    "            f\"Analysts upgrade {symbol} with buy rating\",\n",
    "            f\"{symbol} expands market presence significantly\"\n",
    "        ]\n",
    "        \n",
    "        negative_templates = [\n",
    "            f\"{symbol} faces regulatory challenges\",\n",
    "            f\"Supply chain issues impact {symbol} operations\",\n",
    "            f\"{symbol} reports disappointing quarterly results\",\n",
    "            f\"Market volatility affects {symbol} performance\",\n",
    "            f\"Competition intensifies for {symbol}\"\n",
    "        ]\n",
    "        \n",
    "        neutral_templates = [\n",
    "            f\"{symbol} announces routine quarterly meeting\",\n",
    "            f\"{symbol} maintains steady market position\",\n",
    "            f\"Industry experts analyze {symbol} trends\",\n",
    "            f\"{symbol} continues operational activities\",\n",
    "            f\"Market watch: {symbol} trading update\"\n",
    "        ]\n",
    "        \n",
    "        # Random selection with realistic distribution\n",
    "        all_templates = positive_templates * 2 + negative_templates + neutral_templates * 3\n",
    "        return np.random.choice(all_templates, size=min(10, len(all_templates)), replace=False).tolist()\n",
    "    \n",
    "    def predict_stock_price(self, symbol, prediction_days=[1, 3, 7], confidence_threshold=0.7):\n",
    "        \"\"\"\n",
    "        Comprehensive stock price prediction with confidence scores\n",
    "        \n",
    "        Args:\n",
    "            symbol (str): Stock symbol\n",
    "            prediction_days (list): Days ahead to predict\n",
    "            confidence_threshold (float): Minimum confidence for strong recommendations\n",
    "            \n",
    "        Returns:\n",
    "            dict: Complete prediction results\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ”® PREDICTING STOCK PRICE FOR {symbol}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Get company information\n",
    "        try:\n",
    "            stock = yf.Ticker(symbol)\n",
    "            info = stock.info\n",
    "            company_name = info.get('longName', symbol)\n",
    "            sector = info.get('sector', 'Unknown')\n",
    "            current_price = info.get('currentPrice', 0)\n",
    "            \n",
    "            if current_price == 0:\n",
    "                # Fallback to recent data\n",
    "                recent_data = self.fetch_stock_data(symbol, period=\"5d\")\n",
    "                current_price = recent_data['Close'].iloc[-1] if recent_data is not None else 0\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting company info: {e}\")\n",
    "            company_name = symbol\n",
    "            sector = \"Unknown\"\n",
    "            current_price = 0\n",
    "        \n",
    "        # Fetch recent stock data (3 months for technical indicators)\n",
    "        print(f\"ðŸ“Š Fetching recent data for {company_name}...\")\n",
    "        stock_data = self.fetch_stock_data(symbol, period=\"6mo\")\n",
    "        \n",
    "        if stock_data is None or len(stock_data) < 50:\n",
    "            return {\"error\": \"Insufficient stock data for prediction\"}\n",
    "        \n",
    "        # Update current price if not available\n",
    "        if current_price == 0:\n",
    "            current_price = stock_data['Close'].iloc[-1]\n",
    "        \n",
    "        # Fetch and analyze recent news\n",
    "        print(f\"ðŸ“° Analyzing recent news sentiment...\")\n",
    "        recent_headlines = self.fetch_recent_news(symbol, company_name)\n",
    "        sentiment_analysis = self.advanced_sentiment_analysis(recent_headlines)\n",
    "        \n",
    "        # Prepare prediction data\n",
    "        prediction_data = self.prepare_comprehensive_training_data(\n",
    "            stock_data.copy(), sentiment_analysis\n",
    "        )\n",
    "        \n",
    "        if len(prediction_data) == 0:\n",
    "            return {\"error\": \"Unable to prepare prediction data\"}\n",
    "        \n",
    "        predictions = {}\n",
    "        \n",
    "        # Make predictions for each time horizon\n",
    "        for days in prediction_days:\n",
    "            model_key = f'{days}d'\n",
    "            \n",
    "            if model_key not in self.models or model_key not in self.scalers:\n",
    "                print(f\"âš ï¸ Model for {days}-day prediction not available. Training required.\")\n",
    "                continue\n",
    "            \n",
    "            # Get the latest feature values\n",
    "            latest_features = prediction_data[self.feature_columns].iloc[-1:].values\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = self.scalers[model_key]\n",
    "            latest_features_scaled = scaler.transform(latest_features)\n",
    "            \n",
    "            # Get predictions from all models\n",
    "            model_predictions = {}\n",
    "            model_confidences = {}\n",
    "            \n",
    "            for model_name, model in self.models[model_key].items():\n",
    "                pred_price = model.predict(latest_features_scaled)[0]\n",
    "                \n",
    "                # Calculate confidence based on historical performance\n",
    "                if hasattr(self, 'evaluation_results') and model_key in self.evaluation_results:\n",
    "                    model_r2 = self.evaluation_results[model_key]['model_results'][model_name]['test_metrics']['r2']\n",
    "                    confidence = max(0.1, min(0.95, model_r2))  # Clamp between 0.1 and 0.95\n",
    "                else:\n",
    "                    confidence = 0.5  # Default confidence\n",
    "                \n",
    "                model_predictions[model_name] = pred_price\n",
    "                model_confidences[model_name] = confidence\n",
    "            \n",
    "            # Weighted ensemble prediction\n",
    "            weights = np.array(list(model_confidences.values()))\n",
    "            weights = weights / weights.sum()  # Normalize weights\n",
    "            \n",
    "            ensemble_prediction = np.average(list(model_predictions.values()), weights=weights)\n",
    "            ensemble_confidence = np.average(list(model_confidences.values()), weights=weights)\n",
    "            \n",
    "            # Calculate prediction metrics\n",
    "            price_change = ensemble_prediction - current_price\n",
    "            price_change_percent = (price_change / current_price) * 100\n",
    "            \n",
    "            # Determine recommendation\n",
    "            if ensemble_confidence >= confidence_threshold:\n",
    "                if price_change_percent > 2:\n",
    "                    recommendation = \"STRONG BUY\"\n",
    "                elif price_change_percent > 0.5:\n",
    "                    recommendation = \"BUY\"\n",
    "                elif price_change_percent < -2:\n",
    "                    recommendation = \"STRONG SELL\"\n",
    "                elif price_change_percent < -0.5:\n",
    "                    recommendation = \"SELL\"\n",
    "                else:\n",
    "                    recommendation = \"HOLD\"\n",
    "            else:\n",
    "                recommendation = f\"HOLD (Low Confidence: {ensemble_confidence:.2f})\"\n",
    "            \n",
    "            predictions[f'{days}_day'] = {\n",
    "                'predicted_price': ensemble_prediction,\n",
    "                'price_change': price_change,\n",
    "                'price_change_percent': price_change_percent,\n",
    "                'confidence': ensemble_confidence,\n",
    "                'recommendation': recommendation,\n",
    "                'model_predictions': model_predictions,\n",
    "                'model_confidences': model_confidences\n",
    "            }\n",
    "        \n",
    "        # Compile final results\n",
    "        results = {\n",
    "            'symbol': symbol,\n",
    "            'company_name': company_name,\n",
    "            'sector': sector,\n",
    "            'current_price': current_price,\n",
    "            'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'predictions': predictions,\n",
    "            'sentiment_analysis': {\n",
    "                'overall_sentiment': sentiment_analysis['avg_polarity'],\n",
    "                'confidence': sentiment_analysis['avg_confidence'],\n",
    "                'positive_news': sentiment_analysis['positive_count'],\n",
    "                'negative_news': sentiment_analysis['negative_count'],\n",
    "                'neutral_news': sentiment_analysis['neutral_count'],\n",
    "                'news_headlines': recent_headlines[:10],  # Top 10 headlines\n",
    "                'sentiment_momentum': sentiment_analysis['sentiment_momentum']\n",
    "            },\n",
    "            'technical_indicators': self.get_current_technical_indicators(stock_data),\n",
    "            'risk_assessment': self.assess_prediction_risk(predictions, sentiment_analysis)\n",
    "        }\n",
    "        \n",
    "        self.print_prediction_summary(results)\n",
    "        return results\n",
    "    \n",
    "    def get_current_technical_indicators(self, stock_data):\n",
    "        \"\"\"Get current technical indicator values\"\"\"\n",
    "        try:\n",
    "            enhanced_data = self.create_advanced_technical_indicators(stock_data)\n",
    "            latest = enhanced_data.iloc[-1]\n",
    "            \n",
    "            return {\n",
    "                'RSI': latest.get('RSI', 0),\n",
    "                'MACD': latest.get('MACD', 0),\n",
    "                'MACD_Signal': latest.get('MACD_Signal', 0),\n",
    "                'BB_Position': latest.get('BB_Position', 0),\n",
    "                'Volume_Ratio': latest.get('Volume_Ratio', 1),\n",
    "                'Price_Volatility': latest.get('Price_Volatility', 0),\n",
    "                'ATR': latest.get('ATR', 0)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating technical indicators: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def assess_prediction_risk(self, predictions, sentiment_analysis):\n",
    "        \"\"\"Assess the risk level of predictions\"\"\"\n",
    "        risk_factors = []\n",
    "        risk_score = 0\n",
    "        \n",
    "        # Sentiment risk\n",
    "        if sentiment_analysis['negative_count'] > sentiment_analysis['positive_count']:\n",
    "            risk_factors.append(\"Negative news sentiment\")\n",
    "            risk_score += 0.2\n",
    "        \n",
    "        if sentiment_analysis['volatility'] > 0.3:\n",
    "            risk_factors.append(\"High sentiment volatility\")\n",
    "            risk_score += 0.1\n",
    "        \n",
    "        # Prediction confidence risk\n",
    "        avg_confidence = np.mean([pred['confidence'] for pred in predictions.values()])\n",
    "        if avg_confidence < 0.6:\n",
    "            risk_factors.append(\"Low prediction confidence\")\n",
    "            risk_score += 0.3\n",
    "        \n",
    "        # Price volatility risk\n",
    "        price_changes = [abs(pred['price_change_percent']) for pred in predictions.values()]\n",
    "        if max(price_changes) > 5:\n",
    "            risk_factors.append(\"High predicted volatility\")\n",
    "            risk_score += 0.2\n",
    "        \n",
    "        # Risk level classification\n",
    "        if risk_score < 0.3:\n",
    "            risk_level = \"LOW\"\n",
    "        elif risk_score < 0.6:\n",
    "            risk_level = \"MEDIUM\"\n",
    "        else:\n",
    "            risk_level = \"HIGH\"\n",
    "        \n",
    "        return {\n",
    "            'risk_level': risk_level,\n",
    "            'risk_score': min(risk_score, 1.0),\n",
    "            'risk_factors': risk_factors,\n",
    "            'recommendation': self.get_risk_recommendation(risk_level)\n",
    "        }\n",
    "    \n",
    "    def get_risk_recommendation(self, risk_level):\n",
    "        \"\"\"Get risk-based recommendation\"\"\"\n",
    "        recommendations = {\n",
    "            'LOW': \"Good conditions for trading with standard position sizing\",\n",
    "            'MEDIUM': \"Exercise caution, consider reduced position sizes\",\n",
    "            'HIGH': \"High risk detected, consider waiting or use very small positions\"\n",
    "        }\n",
    "        return recommendations.get(risk_level, \"Assess carefully before trading\")\n",
    "    \n",
    "    def print_prediction_summary(self, results):\n",
    "        \"\"\"Print a comprehensive prediction summary\"\"\"\n",
    "        print(f\"\\nðŸŽ¯ PREDICTION RESULTS FOR {results['company_name']} ({results['symbol']})\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Sector: {results['sector']}\")\n",
    "        print(f\"Current Price: ${results['current_price']:.2f}\")\n",
    "        print(f\"Analysis Date: {results['analysis_date']}\")\n",
    "        \n",
    "        print(f\"\\nðŸ“Š PRICE PREDICTIONS:\")\n",
    "        print(\"-\" * 40)\n",
    "        for period, pred in results['predictions'].items():\n",
    "            days = period.replace('_day', '-day')\n",
    "            print(f\"{days.upper()}:\")\n",
    "            print(f\"  Predicted Price: ${pred['predicted_price']:.2f}\")\n",
    "            print(f\"  Price Change: ${pred['price_change']:.2f} ({pred['price_change_percent']:.2f}%)\")\n",
    "            print(f\"  Confidence: {pred['confidence']:.2f}\")\n",
    "            print(f\"  Recommendation: {pred['recommendation']}\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"ðŸ“° NEWS SENTIMENT ANALYSIS:\")\n",
    "        print(\"-\" * 40)\n",
    "        sentiment = results['sentiment_analysis']\n",
    "        print(f\"Overall Sentiment: {sentiment['overall_sentiment']:.3f} ({'Positive' if sentiment['overall_sentiment'] > 0 else 'Negative' if sentiment['overall_sentiment'] < 0 else 'Neutral'})\")\n",
    "        print(f\"News Distribution: {sentiment['positive_news']} Positive, {sentiment['negative_news']} Negative, {sentiment['neutral_news']} Neutral\")\n",
    "        print(f\"Sentiment Momentum: {sentiment['sentiment_momentum']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nðŸ” TOP NEWS HEADLINES:\")\n",
    "        for i, headline in enumerate(sentiment['news_headlines'][:5], 1):\n",
    "            print(f\"  {i}. {headline}\")\n",
    "        \n",
    "        print(f\"\\nâš ï¸ RISK ASSESSMENT:\")\n",
    "        print(\"-\" * 40)\n",
    "        risk = results['risk_assessment']\n",
    "        print(f\"Risk Level: {risk['risk_level']}\")\n",
    "        print(f\"Risk Score: {risk['risk_score']:.2f}/1.00\")\n",
    "        print(f\"Risk Factors: {', '.join(risk['risk_factors']) if risk['risk_factors'] else 'None identified'}\")\n",
    "        print(f\"Recommendation: {risk['recommendation']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fc4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Training and Evaluation Pipeline\n",
    "def run_complete_pipeline():\n",
    "    \"\"\"\n",
    "    Run the complete pipeline: data loading, training, evaluation, and prediction\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ STARTING COMPREHENSIVE STOCK PREDICTION PIPELINE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize predictor\n",
    "    predictor = AdvancedStockPredictor(use_advanced_sentiment=True)\n",
    "    \n",
    "    # Load datasets from Hugging Face (if available)\n",
    "    print(\"\\nðŸ“š Loading datasets from Hugging Face...\")\n",
    "    hf_datasets = predictor.load_huggingface_datasets()\n",
    "    \n",
    "    # Train on multiple stocks for robust model\n",
    "    training_symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN']\n",
    "    prediction_horizons = [1, 3, 7]\n",
    "    \n",
    "    all_training_data = []\n",
    "    \n",
    "    for symbol in training_symbols:\n",
    "        print(f\"\\nðŸ“ˆ Processing {symbol}...\")\n",
    "        \n",
    "        # Fetch stock data\n",
    "        stock_data = predictor.fetch_stock_data(symbol, period=\"2y\")\n",
    "        if stock_data is None:\n",
    "            continue\n",
    "        \n",
    "        # Get news and sentiment\n",
    "        headlines = predictor.fetch_recent_news(symbol, symbol)\n",
    "        sentiment = predictor.advanced_sentiment_analysis(headlines)\n",
    "        \n",
    "        # Prepare training data for each horizon\n",
    "        for days in prediction_horizons:\n",
    "            training_data = predictor.prepare_comprehensive_training_data(\n",
    "                stock_data.copy(), sentiment, target_days=days\n",
    "            )\n",
    "            \n",
    "            if len(training_data) > 0:\n",
    "                training_data['symbol'] = symbol\n",
    "                training_data['target_days'] = days\n",
    "                all_training_data.append(training_data)\n",
    "    \n",
    "    # Train models for each prediction horizon\n",
    "    for days in prediction_horizons:\n",
    "        print(f\"\\nðŸŽ¯ Training models for {days}-day prediction...\")\n",
    "        \n",
    "        # Combine data from all stocks for this horizon\n",
    "        horizon_data = pd.concat([\n",
    "            df for df in all_training_data \n",
    "            if not df.empty and df['target_days'].iloc[0] == days\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        if len(horizon_data) > 100:  # Ensure sufficient data\n",
    "            # Train ensemble\n",
    "            training_results = predictor.train_ensemble_models(\n",
    "                horizon_data, target_days=days, test_size=0.2\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            predictor.evaluation_results[f'{days}d'] = training_results\n",
    "            \n",
    "            # Create visualizations\n",
    "            predictor.create_evaluation_visualizations(\n",
    "                training_results, symbol=f\"Multi-Stock {days}d\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Insufficient data for {days}-day model training\")\n",
    "    \n",
    "    # Demonstrate predictions on new stocks\n",
    "    test_symbols = ['NVDA', 'META', 'NFLX']\n",
    "    \n",
    "    print(f\"\\nðŸ”® MAKING PREDICTIONS ON TEST STOCKS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for symbol in test_symbols:\n",
    "        try:\n",
    "            prediction_results = predictor.predict_stock_price(\n",
    "                symbol, \n",
    "                prediction_days=prediction_horizons,\n",
    "                confidence_threshold=0.6\n",
    "            )\n",
    "            \n",
    "            if 'error' not in prediction_results:\n",
    "                # Save results\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                filename = f'{symbol}_prediction_{timestamp}.json'\n",
    "                \n",
    "                # Convert numpy types to native Python types for JSON serialization\n",
    "                json_results = json.loads(json.dumps(prediction_results, default=str))\n",
    "                \n",
    "                with open(filename, 'w') as f:\n",
    "                    json.dump(json_results, f, indent=2, default=str)\n",
    "                \n",
    "                print(f\"âœ… Results saved to {filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error predicting {symbol}: {e}\")\n",
    "    \n",
    "    print(f\"\\nâœ… PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"Check the generated visualizations and JSON files for detailed results.\")\n",
    "    \n",
    "    return predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2016a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Instructions and Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\"\"\n",
    "ðŸŒŸ ADVANCED STOCK PREDICTION PIPELINE\n",
    "=====================================\n",
    "\n",
    "This enhanced pipeline includes:\n",
    "\n",
    "âœ… FEATURES IMPLEMENTED:\n",
    "- Train/Test split with proper time series validation\n",
    "- Comprehensive evaluation metrics (MAE, MSE, RMSE, RÂ², MAPE)\n",
    "- 12 detailed visualizations for model analysis\n",
    "- Integration with Hugging Face financial datasets\n",
    "- Advanced sentiment analysis (FinBERT when available)\n",
    "- Multi-horizon predictions (1, 3, 7 days)\n",
    "- Ensemble modeling with confidence scores\n",
    "- Risk assessment and recommendations\n",
    "- Complete data pipeline (aggregation â†’ cleaning â†’ training â†’ evaluation)\n",
    "- Automated prediction service with news integration\n",
    "\n",
    "ðŸ“Š EVALUATION METRICS:\n",
    "- RÂ² Score (coefficient of determination)\n",
    "- Mean Absolute Error (MAE) in dollars\n",
    "- Root Mean Square Error (RMSE)\n",
    "- Mean Absolute Percentage Error (MAPE)\n",
    "- Cross-validation scores\n",
    "\n",
    "ðŸ” VISUALIZATIONS INCLUDED:\n",
    "1. Model RÂ² Comparison\n",
    "2. Model MAE Comparison  \n",
    "3. Feature Importance Analysis\n",
    "4. Prediction vs Actual Scatter Plot\n",
    "5. Residuals Analysis\n",
    "6. Time Series Comparison\n",
    "7. Error Distribution Histogram\n",
    "8. Cross-Validation Box Plots\n",
    "9. Learning Curve Analysis\n",
    "10. Metrics Heatmap\n",
    "11. Accuracy by Price Range\n",
    "12. Prediction Confidence Distribution\n",
    "\n",
    "ðŸ“š SUPPORTED DATASETS:\n",
    "- FNSPID: 29M+ financial records with news sentiment\n",
    "- Financial News Dataset: Real financial news headlines\n",
    "- Financial Phrase Bank: Sentiment-labeled financial text\n",
    "- Your custom datasets (CSV format supported)\n",
    "\n",
    "ðŸŽ¯ PREDICTION FEATURES:\n",
    "- Multi-day predictions (1, 3, 7 days)\n",
    "- Confidence scores for each prediction\n",
    "- Buy/Sell/Hold recommendations\n",
    "- Risk assessment with factors\n",
    "- News headline analysis and sentiment\n",
    "- Technical indicator integration\n",
    "- Real-time news fetching (with API key)\n",
    "\n",
    "INSTALLATION:\n",
    "pip install pandas numpy scikit-learn matplotlib seaborn plotly yfinance textblob requests datasets transformers\n",
    "\n",
    "USAGE:\n",
    "# Basic training and prediction\n",
    "predictor = run_complete_pipeline()\n",
    "\n",
    "# Individual stock prediction\n",
    "predictor = AdvancedStockPredictor()\n",
    "# ... train your models first ...\n",
    "result = predictor.predict_stock_price('AAPL', prediction_days=[1,3,7])\n",
    "\n",
    "DATASETS ON HUGGING FACE:\n",
    "- Zihan1004/FNSPID (Financial News Sentiment and Price Impact)\n",
    "- ashraq/financial-news (Financial news headlines)\n",
    "- takala/financial_phrasebank (Financial sentiment analysis)\n",
    "\n",
    "    \n",
    "    # Execute the complete pipeline\n",
    "    try:\n",
    "        pipeline_results = run_complete_pipeline()\n",
    "        print(f\"\\nðŸŽ‰ Pipeline execution completed successfully!\")\n",
    "        print(f\"Trained models available for prediction horizons: {list(pipeline_results.models.keys())}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Pipeline execution failed: {e}\")\n",
    "        print(\"Try running individual components for debugging:\")\n",
    "        print(\"predictor = AdvancedStockPredictor()\")\n",
    "        print(\"stock_data = predictor.fetch_stock_data('AAPL', period='1y')\")\n",
    "        print(\"# ... continue with manual training ...\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4317768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "predictor = run_complete_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32394a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "result = predictor.predict_stock_price('AAPL', prediction_days=[1,3,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adbe173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Specify the filename to save the models\n",
    "model_filename = 'advanced_stock_predictor_models.pkl'\n",
    "\n",
    "# Check if the predictor object exists and has trained models\n",
    "if 'predictor' in locals() and hasattr(predictor, 'models') and predictor.models:\n",
    "    try:\n",
    "        # Save the models to the specified file\n",
    "        joblib.dump(predictor.models, model_filename)\n",
    "        print(f\"Trained models saved successfully to '{model_filename}'\")\n",
    "\n",
    "        # You can also save the scalers if needed for future predictions\n",
    "        scaler_filename = 'advanced_stock_predictor_scalers.pkl'\n",
    "        joblib.dump(predictor.scalers, scaler_filename)\n",
    "        print(f\"Trained scalers saved successfully to '{scaler_filename}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving models: {e}\")\n",
    "else:\n",
    "    print(\"No trained models found. Please run the training pipeline first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a6e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instructions for loading and using models in another notebook ---\n",
    "\n",
    "# To load the saved models and scalers in another notebook, you can use the following code:\n",
    "# Make sure you have the 'advanced_stock_predictor_models.pkl' and 'advanced_stock_predictor_scalers.pkl' files in the same directory as your notebook or provide the correct path.\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load the models and scalers\n",
    "try:\n",
    "    loaded_models = joblib.load('advanced_stock_predictor_models.pkl')\n",
    "    loaded_scalers = joblib.load('advanced_stock_predictor_scalers.pkl')\n",
    "    print(\"Models and scalers loaded successfully.\")\n",
    "\n",
    "    # Create a new predictor instance and assign the loaded models and scalers\n",
    "    # Note: You might need to re-initialize some components like the sentiment analyzer\n",
    "    # depending on your AdvancedStockPredictor class implementation.\n",
    "    new_predictor = AdvancedStockPredictor() # Initialize with necessary parameters\n",
    "    new_predictor.models = loaded_models\n",
    "    new_predictor.scalers = loaded_scalers\n",
    "    technical_features = [\n",
    "            'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "            'SMA_5', 'SMA_10', 'SMA_20', 'SMA_50',\n",
    "            'EMA_12', 'EMA_26', 'MACD', 'MACD_Signal', 'MACD_Histogram',\n",
    "            'RSI', 'BB_Upper', 'BB_Lower', 'BB_Width', 'BB_Position',\n",
    "            'Volume_Ratio', 'Price_Change', 'Price_Change_5d', 'Price_Volatility',\n",
    "            'Stochastic_%K', 'Stochastic_%D', 'Williams_%R', 'ATR', 'CCI', 'OBV', 'ROC'\n",
    "        ]\n",
    "        \n",
    "    sentiment_features = [\n",
    "            'sentiment_avg_polarity', 'sentiment_polarity_std', 'sentiment_avg_confidence',\n",
    "            'sentiment_positive_count', 'sentiment_negative_count', 'sentiment_neutral_count',\n",
    "            'sentiment_sentiment_momentum', 'sentiment_volatility'\n",
    "        ]\n",
    "        \n",
    "    lag_features = [f'Close_lag_{lag}' for lag in [1, 2, 3, 5]]\n",
    "    lag_features += [f'Volume_lag_{lag}' for lag in [1, 2, 3, 5]]\n",
    "    lag_features += [f'RSI_lag_{lag}' for lag in [1, 2, 3, 5]]\n",
    "        \n",
    "    rolling_features = []\n",
    "    for window in [5, 10, 20]:\n",
    "        rolling_features += [f'Close_std_{window}', f'Volume_std_{window}']\n",
    "        \n",
    "    new_predictor.feature_columns = technical_features + sentiment_features + lag_features + rolling_features\n",
    "\n",
    "    print(\"Predictor instance updated with loaded models.\")\n",
    "\n",
    "    # Now you can use new_predictor to make predictions\n",
    "    # Example:\n",
    "    # prediction_results = new_predictor.predict_stock_price('MSFT', prediction_days=[1, 3, 7])\n",
    "    # print(prediction_results)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Model or scaler file not found. Make sure 'advanced_stock_predictor_models.pkl' and 'advanced_stock_predictor_scalers.pkl' are in the correct directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8059fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "prediction_results = new_predictor.predict_stock_price('MSFT', prediction_days=[1, 3, 7])\n",
    "print(prediction_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
