{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "XbFlZyjs1DQw"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Install dependencies\n",
        "# ================================\n",
        "%pip install --quiet yfinance pandas numpy scikit-learn tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "8lTPqlvHEyZV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Imports\n",
        "# -----------------------------\n",
        "import os\n",
        "import json\n",
        "import concurrent.futures\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import inspect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Dcv9dylxEyZW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Parameters\n",
        "# -----------------------------\n",
        "SEQ_LEN = 20\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "MAX_WORKERS = 8\n",
        "DATA_HISTORY_IN_YEARS = 20\n",
        "DATA_INTERVAL_IN_DAYS = 1\n",
        "DATA_DIR = \"data\"\n",
        "FEATURES = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
        "MODEL_NAME = \"nifty100_lstm_model_v2.keras\"\n",
        "DATA_DICT_FILE = \"data_dict.json\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "cgw2dMNnEyZW"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Training Stocks\n",
        "# -----------------------------\n",
        "symbols = [\n",
        "    \"ADANIPORTS.NS\", \"ASIANPAINT.NS\", \"AXISBANK.NS\", \"BAJAJ-AUTO.NS\", \"BAJFINANCE.NS\",\n",
        "    \"BAJAJFINSV.NS\", \"BPCL.NS\", \"BHARTIARTL.NS\", \"BRITANNIA.NS\", \"CIPLA.NS\",\n",
        "    \"COALINDIA.NS\", \"DRREDDY.NS\", \"EICHERMOT.NS\", \"GAIL.NS\", \"GRASIM.NS\",\n",
        "    \"HCLTECH.NS\", \"HDFCBANK.NS\", \"HDFCLIFE.NS\", \"ICICIBANK.NS\", \"INDUSINDBK.NS\",\n",
        "    \"INFY.NS\", \"ITC.NS\", \"JSWSTEEL.NS\", \"KOTAKBANK.NS\", \"LT.NS\",\n",
        "    \"M&M.NS\", \"MARUTI.NS\", \"NTPC.NS\", \"NESTLEIND.NS\", \"ONGC.NS\",\n",
        "    \"POWERGRID.NS\", \"RELIANCE.NS\", \"SBILIFE.NS\", \"SBIN.NS\", \"SUNPHARMA.NS\",\n",
        "    \"TCS.NS\", \"TATACONSUM.NS\", \"TATASTEEL.NS\", \"TECHM.NS\", \"TITAN.NS\",\n",
        "    \"ULTRACEMCO.NS\", \"UPL.NS\", \"WIPRO.NS\", \"DIVISLAB.NS\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "4_f07U-kEyZW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Data Fetcher (cached download)\n",
        "# -----------------------------\n",
        "\n",
        "def fetch_stock(sym):\n",
        "    file_path = os.path.join(DATA_DIR, f\"{sym}.csv\")\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        # Load existing CSV\n",
        "        df_existing = pd.read_csv(file_path, parse_dates=[\"Date\"])\n",
        "\n",
        "        # Find the last available date\n",
        "        last_date = df_existing[\"Date\"].max()\n",
        "\n",
        "        start_date = (last_date + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        # if start_date is greater than today then return existing df\n",
        "        if pd.to_datetime(start_date) > pd.to_datetime(\"today\"):\n",
        "            return sym, df_existing\n",
        "\n",
        "\n",
        "        # Fetch new data starting from the day after last_date\n",
        "        df_new = yf.download(\n",
        "            sym,\n",
        "            start=(last_date + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\"),\n",
        "            interval=f\"{DATA_INTERVAL_IN_DAYS}d\"\n",
        "        )\n",
        "\n",
        "        if not df_new.empty:\n",
        "            df_new = df_new[FEATURES].dropna()\n",
        "\n",
        "            # Flatten MultiIndex if present\n",
        "            if isinstance(df_new.columns, pd.MultiIndex):\n",
        "                df_new.columns = df_new.columns.get_level_values(0)\n",
        "\n",
        "            df_new = df_new.reset_index()\n",
        "\n",
        "            # Append only new rows\n",
        "            df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
        "            df_combined = df_combined.drop_duplicates(subset=[\"Date\"]).reset_index(drop=True)\n",
        "\n",
        "            # Save updated CSV\n",
        "            df_combined.to_csv(file_path, index=False)\n",
        "            return sym, df_combined\n",
        "\n",
        "        # If no new data, return existing\n",
        "        return sym, df_existing\n",
        "\n",
        "    else:\n",
        "        # No CSV yet — fetch full history\n",
        "        df = yf.download(\n",
        "            sym,\n",
        "            period=f\"{DATA_HISTORY_IN_YEARS}y\",\n",
        "            interval=f\"{DATA_INTERVAL_IN_DAYS}d\"\n",
        "        )\n",
        "        if df.empty:\n",
        "            return sym, None\n",
        "\n",
        "        df = df[FEATURES].dropna()\n",
        "\n",
        "        if isinstance(df.columns, pd.MultiIndex):\n",
        "            df.columns = df.columns.get_level_values(0)\n",
        "\n",
        "        df = df.reset_index()\n",
        "        df.to_csv(file_path, index=False)\n",
        "        return sym, df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "K-WUSwx3EyZX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Feature Engineering\n",
        "# -----------------------------\n",
        "def add_features(df: pd.DataFrame, drop_date: bool = False):\n",
        "    df = df.copy()\n",
        "    df[\"Return\"] = df[\"Close\"].pct_change()\n",
        "    df[\"MA7\"] = df[\"Close\"].rolling(7).mean()\n",
        "    df[\"MA21\"] = df[\"Close\"].rolling(21).mean()\n",
        "    df[\"STD21\"] = df[\"Close\"].rolling(21).std()\n",
        "    df[\"Upper_BB\"] = df[\"MA21\"] + (df[\"STD21\"] * 2)\n",
        "    df[\"Lower_BB\"] = df[\"MA21\"] - (df[\"STD21\"] * 2)\n",
        "    df[\"EMA\"] = df[\"Close\"].ewm(span=20, adjust=False).mean()\n",
        "    df[\"Momentum\"] = df[\"Close\"] - df[\"Close\"].shift(5)\n",
        "    if drop_date:\n",
        "        df = df.drop(columns=[\"Date\"])\n",
        "    return df.dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "5KG7P04VEyZX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Sequence Creator\n",
        "# -----------------------------\n",
        "def create_sequences(data, seq_len=SEQ_LEN, split=0.8):\n",
        "    X, y = [], []\n",
        "    for i in range(seq_len, len(data)):\n",
        "        X.append(data[i-seq_len:i])\n",
        "        y.append(data[i, 3])  # 3 = Close column\n",
        "    X, y = np.array(X), np.array(y).reshape(-1,1)\n",
        "    split_idx = int(len(X) * split)\n",
        "    return (X[:split_idx], y[:split_idx]), (X[split_idx:], y[split_idx:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "RUVuvAeTEyZX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Build Model\n",
        "# -----------------------------\n",
        "def build_model(input_shape):\n",
        "    model = Sequential([\n",
        "        LSTM(128, return_sequences=True, input_shape=input_shape),\n",
        "        Dropout(0.4),\n",
        "        LSTM(64),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(0.001), loss=Huber())\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hChdG69gEyZY",
        "outputId": "cf52b049-868b-4093-feb4-ee998fe2a7ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded existing data_dict from data_dict.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching data: 100%|██████████| 44/44 [00:00<00:00, 63.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved updated data_dict to data_dict.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Training Pipeline\n",
        "# -----------------------------\n",
        "\n",
        "def save_data_dict(data_dict, file_path=DATA_DICT_FILE):\n",
        "    # Convert DataFrames to a JSON-friendly format (dictionary of lists)\n",
        "    json_data_dict = {}\n",
        "    for symbol, df in data_dict.items():\n",
        "        # Convert DataFrame to dictionary of lists, handling datetimes\n",
        "        json_data_dict[symbol] = df.to_dict(orient='list')\n",
        "        # Convert datetime objects to strings for JSON compatibility\n",
        "        if 'Date' in json_data_dict[symbol]:\n",
        "            json_data_dict[symbol]['Date'] = [str(date) for date in json_data_dict[symbol]['Date']]\n",
        "    # Dump the dictionary to a JSON file\n",
        "    with open(file_path, \"w\") as f:\n",
        "        json.dump(json_data_dict, f, indent=4)\n",
        "    print(f\"✅ Saved updated data_dict to {file_path}\")\n",
        "\n",
        "def load_data_dict(file_path=DATA_DICT_FILE):\n",
        "    if not os.path.exists(file_path):\n",
        "        return {}\n",
        "    with open(file_path, \"r\") as f:\n",
        "        loaded = json.load(f)\n",
        "    data_dict = {}\n",
        "    for sym, data in loaded.items():\n",
        "        df = pd.DataFrame(data)\n",
        "        if \"Date\" in df.columns:\n",
        "            df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "        data_dict[sym] = df\n",
        "    print(f\"✅ Loaded existing data_dict from {file_path}\")\n",
        "    return data_dict\n",
        "\n",
        "def update_data_dict(symbols):\n",
        "    # Load existing dictionary if present\n",
        "    data_dict = load_data_dict()\n",
        "\n",
        "    # Fetch in parallel\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        results = list(tqdm(executor.map(fetch_stock, symbols), total=len(symbols), desc=\"Fetching data\"))\n",
        "\n",
        "    for sym, df in results:\n",
        "        if df is not None:\n",
        "            if sym in data_dict:\n",
        "                # Append only new rows\n",
        "                df_existing = data_dict[sym]\n",
        "                df_combined = pd.concat([df_existing, df], ignore_index=True)\n",
        "                df_combined = df_combined.drop_duplicates(subset=[\"Date\"]).reset_index(drop=True)\n",
        "                data_dict[sym] = add_features(df_combined)\n",
        "            else:\n",
        "                data_dict[sym] = add_features(df)\n",
        "\n",
        "    # Save back\n",
        "    save_data_dict(data_dict)\n",
        "    return data_dict\n",
        "\n",
        "\n",
        "data_dict = update_data_dict(symbols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "RMSG4p2UEyZY"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Scale the Data\n",
        "# -----------------------------\n",
        "\n",
        "scaler_dict, scaled_data_dict = {}, {}\n",
        "for sym, df in data_dict.items():\n",
        "    df = df.drop(columns=[\"Date\"])\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled = scaler.fit_transform(df)\n",
        "    scaler_dict[sym] = scaler\n",
        "    scaled_data_dict[sym] = scaled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "i7omNm8FEyZY"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Prepare Training Data\n",
        "# -----------------------------\n",
        "X_train_list, y_train_list, X_test_list, y_test_list = [], [], [], []\n",
        "for scaled in scaled_data_dict.values():\n",
        "    (X_tr, y_tr), (X_te, y_te) = create_sequences(scaled, SEQ_LEN)\n",
        "    X_train_list.append(X_tr); y_train_list.append(y_tr)\n",
        "    X_test_list.append(X_te);  y_test_list.append(y_te)\n",
        "\n",
        "X_train, y_train = np.vstack(X_train_list), np.vstack(y_train_list)\n",
        "X_test, y_test = np.vstack(X_test_list), np.vstack(y_test_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOvPgq55EyZY",
        "outputId": "cbfd1637-7648-4e94-cd88-4c8302e57fea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Build Model\n",
        "# -----------------------------\n",
        "\n",
        "model = build_model((SEQ_LEN, X_train.shape[2]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_LoX6zQEyZZ",
        "outputId": "04ddf87e-1e6f-42e0-fac6-87410e6cc80f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 48ms/step - loss: 5.0093e-04 - val_loss: 1.0580e-04 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 48ms/step - loss: 1.4614e-04 - val_loss: 1.1870e-04 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 47ms/step - loss: 1.2351e-04 - val_loss: 3.3628e-05 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m4132/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.1809e-04\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 47ms/step - loss: 1.1809e-04 - val_loss: 2.9194e-05 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 46ms/step - loss: 9.8996e-05 - val_loss: 1.7921e-05 - learning_rate: 5.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 47ms/step - loss: 9.5710e-05 - val_loss: 3.8067e-05 - learning_rate: 5.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m4132/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 9.8631e-05\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 48ms/step - loss: 9.8630e-05 - val_loss: 1.6932e-05 - learning_rate: 5.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 48ms/step - loss: 8.9757e-05 - val_loss: 1.7635e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 48ms/step - loss: 9.0523e-05 - val_loss: 1.7889e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 8.6405e-05\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 48ms/step - loss: 8.6405e-05 - val_loss: 4.3056e-05 - learning_rate: 2.5000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 46ms/step - loss: 8.5576e-05 - val_loss: 1.9066e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 47ms/step - loss: 8.3110e-05 - val_loss: 2.2504e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 8.3180e-05\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 46ms/step - loss: 8.3180e-05 - val_loss: 2.3807e-05 - learning_rate: 1.2500e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 46ms/step - loss: 8.3430e-05 - val_loss: 1.8129e-05 - learning_rate: 1.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 46ms/step - loss: 8.0799e-05 - val_loss: 2.9220e-05 - learning_rate: 1.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 47ms/step - loss: 8.0916e-05 - val_loss: 1.9567e-05 - learning_rate: 1.0000e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m4133/4133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 48ms/step - loss: 8.1457e-05 - val_loss: 2.0319e-05 - learning_rate: 1.0000e-04\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Fit or Train the model\n",
        "# -----------------------------\n",
        "\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-4, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Save Model\n",
        "# -----------------------------\n",
        "\n",
        "model.save(MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kILH4S0jEyZZ",
        "outputId": "7d7230ef-228d-4612-954d-c518f8005e29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 14ms/step\n",
            "RMSE: 0.0330, MAE: 0.0116, R²: 0.9827\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluate\n",
        "# -----------------------------\n",
        "model = load_model(MODEL_NAME, compile=False)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# fix RMSE for old sklearn\n",
        "if \"squared\" in inspect.signature(mean_squared_error).parameters:\n",
        "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "else:\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "NRDsrfa9zBes"
      },
      "outputs": [],
      "source": [
        "def predict_stock_simple(symbol, model_path=MODEL_NAME):\n",
        "    \"\"\"Simplified approach - retrain scaler on recent data\"\"\"\n",
        "    model = load_model(model_path, compile=False)\n",
        "\n",
        "    # Fetch data\n",
        "    sym, df = fetch_stock(symbol)\n",
        "    if df is None:\n",
        "        print(f\"❌ No data for {symbol}\")\n",
        "        return None\n",
        "\n",
        "    # Apply feature engineering\n",
        "    df_features = add_features(df, drop_date=True)\n",
        "\n",
        "    # Use only recent data for scaling (e.g., last 200 days)\n",
        "    recent_data = df_features.tail(200)\n",
        "\n",
        "    # Create a fresh scaler on recent data\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_recent = scaler.fit_transform(recent_data)\n",
        "\n",
        "    # Get last SEQ_LEN rows for prediction\n",
        "    X = scaled_recent[-SEQ_LEN:].reshape(1, SEQ_LEN, scaled_recent.shape[1])\n",
        "\n",
        "    # Predict\n",
        "    pred_scaled = model.predict(X, verbose=0)[0][0]\n",
        "\n",
        "    # Inverse transform\n",
        "    last_row = scaled_recent[-1].copy()\n",
        "    last_row[3] = pred_scaled\n",
        "    pred_actual = scaler.inverse_transform([last_row])[0, 3]\n",
        "\n",
        "    last_close = df_features[\"Close\"].iloc[-1]\n",
        "\n",
        "    return {\n",
        "        \"symbol\": symbol,\n",
        "        \"last_close\": float(last_close),\n",
        "        \"predicted_close\": float(pred_actual)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onsVdd3cxhkW",
        "outputId": "40ae9dcb-9b5c-413c-e713-a06bce12545d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'symbol': 'TCS.NS', 'last_close': 3020.89990234375, 'predicted_close': 3017.987731115834}\n"
          ]
        }
      ],
      "source": [
        "print(predict_stock_simple(\"TCS.NS\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
